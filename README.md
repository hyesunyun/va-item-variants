# Keeping Users Engaged During Repeated Interviews by a Virtual Agent: Using Large Language Models to Reliably Diversify Questions

This GitHub repository provides the supplementary materials.
As part of the supplementary materials, we include the LLM-generated item (question) variants and content we included in the ITEM VARIANTS PLUS condition. Also, we provide the prompts used to generate the item (question) variants and conversational contents. Lastly, we provide details of the response values participants provided when interacting with the agent over the course of 14 days.

## Prompts

You can find the prompts in the [`PROMPTS.md`](PROMPTS.md) file.

## LLM-Generated Content

All contents used in the validation study can be found in the `llm_outputs` folder as csv files.
Please refer to the [`OUTPUTS.md`](llm_outputs/OUTPUTS.md) file for descriptions of the contents.

## Participant Responses

During the two-week validation study, participants answered questions administered by the agent. The [`responses_table.pdf`](responses_table.pdf) provides the percentage of response values for each PROMIS Depression questions across all three conditions. The table also factors in days that the participants did not interact with the agent or skipped to answer.
